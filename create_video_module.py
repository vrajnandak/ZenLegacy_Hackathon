import os
import numpy as np
import pickle

import tqdm
import cv2

from UtilClasses import *

from mediapipe.python.solutions import drawing_utils as mp_drawing
from mediapipe.python.solutions import pose as mp_pose

#Global Variables of the model which have been specified as shown below for best accuracy on the training data we created.
repitition_variables_for_classes=[['PushUps_Down',6,4],['PushUps_Up',6,4],['Squats_Down',6,4],['Squats_Up',6,4],['Deadlift_Down',6,4],['Deadlift_Up',6,4]]
top_n_by_max_distance=30
top_n_by_mean_distance=10

#Maintaining the Rep Counters.
Rep_counters=[RepetitionCounter(class_name,enter_threshold,exit_threshold) for class_name,enter_threshold,exit_threshold in repitition_variables_for_classes]

#csv file generated by creating the embeddings of the training data(i.e., the screenshots we took).
training_screenshots_embeddings_csvs='fitness_poses_csvs_out'

pose_tracker=mp_pose.Pose()
pose_embedder=FullBodyPoseEmbedder()

#The parameter values will be changed accordingly by us(using the screenshots and then we will check accuracies and decide on best parameter values), and then bijeet will tell the best values for these parameters after having tested the following code on his own, accounting for push-ups, squats, deadlift etc.
pose_classifier=PoseClassifier(training_screenshots_embeddings_csvs,pose_embedder, top_n_by_max_distance=top_n_by_max_distance, top_n_by_mean_distance=top_n_by_mean_distance)

pose_classification_filter=EMADictSmoothing(window_size=10, alpha=0.2)

# repetition_counter=RepetitionCounter(class_name=class_name,enter_threshold=6,exit_threshold=4)


def create_video(input_video_file_path,output_video_file_path):

    #Input Video related things.
    input_video_camera=cv2.VideoCapture(input_video_file_path)

    #Total frames in video, FPS of video, width of video, height of input video.
    input_video_total_frames = input_video_camera.get(cv2.CAP_PROP_FRAME_COUNT)              #video_n_frames
    input_video_fps = input_video_camera.get(cv2.CAP_PROP_FPS)
    input_video_width = int(input_video_camera.get(cv2.CAP_PROP_FRAME_WIDTH))
    input_video_height = int(input_video_camera.get(cv2.CAP_PROP_FRAME_HEIGHT))

    #Pose Classification Visualizer being created inside function rather than globally due to the 2nd parameter being dependent on the input video.
   #  pose_classification_visualizer=PoseClassificationVisualizer(class_name=class_name, plot_x_max=input_video_total_frames, plot_y_max=10)
    # PosesClassificationVisualizers=[PoseClassificationVisualizer(class_name,plot_x_max=input_video_total_frames,plot_y_max=10) for class_name,enter,exit in repitition_variables_for_classes]

    #Output Video Writer. Writes the processed frames to the video specified by the path.
    output_video=cv2.VideoWriter(output_video_file_path, cv2.VideoWriter_fourcc(*'mp4v'),input_video_fps,(input_video_width, input_video_height))

    repetition_counter=Rep_counters[0]
    pose_classification_visualizer=PoseClassificationVisualizer('PushUps_Down',plot_x_max=input_video_total_frames,plot_y_max=10)
    #pose_classification_visualizer=PosesClassificationVisualizers[0]
    frame_idx=0
    output_frame=None       #For writing this frame to the output video i.e., the processed video.
    with tqdm.tqdm(total=input_video_total_frames, position=0, leave=True) as pbar:
        while True:
            print("Frame idx ",frame_idx)
            # Get next frame of the video.
            success, input_frame = input_video_camera.read()
            if not success:
                break

            # Run pose tracker.
            input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)
            result = pose_tracker.process(image=input_frame)
            pose_landmarks = result.pose_landmarks

            #Drawing the landmarks on the output frame.
            output_frame=input_frame.copy()
            if pose_landmarks is not None:
                mp_drawing.draw_landmarks(image=output_frame, landmark_list=pose_landmarks, connections=mp_pose.POSE_CONNECTIONS)

            #Classifying the landmarks into one of the classes 'PushUps_Down','PushUps_Up', 'Squats_Up', 'Squats_Down'.
            if pose_landmarks is not None:
                frame_height, frame_width, channel=output_frame.shape       #WE ARE NOT USING THE VARIABLE 'channel'.

                #Scaling the x,y,z values of the landmarks from being percentages with respect to the screen width, height to be the pixel values on the window. z-value represents the landmark depth and the magnitude of z uses roughly the same scale as x, hence we use frame_width for scaling z-value.
                pose_landmarks=np.array([[lmk.x*frame_width, lmk.y*frame_height, lmk.z*frame_width] for lmk in pose_landmarks.landmark], dtype=np.float32)

                #Assert statement to check if all the landmarks for the frame have been identified properly.
                assert pose_landmarks.shape==(33,3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)

                #Classifying the current pose by using the pose_classifier which has been trained with the screenshots.
                pose_classification=pose_classifier(pose_landmarks)     #pose_classification now has a list of class names from the points(of the top_n_closest_embedded_points).
                pose_classification_filtered=pose_classification_filter(pose_classification)        #Smoothening the data.
                repetitions_count=repetition_counter(pose_classification_filtered)                  #Getting the count of times that the pose of 'class_name' has occured. The class_name variable here refers to the class name that the repetition_counter was initialized to.
            else:
                pose_classification=None        #No pose resulting in no classification on the current frame.
                pose_classification_filtered=pose_classification_filter(dict())     #Smoothing for the future frames.
                pose_classification_filtered=None

                #Assuming that the person is 'frozen' as the landmarks are not visible, so the latest value of repetitions count is only taken.
                repetitions_count=repetition_counter.n_repeats

            #Drawing the output frame.
            output_frame=pose_classification_visualizer(frame=output_frame,pose_classification=pose_classification, pose_classification_filtered=pose_classification_filtered, repetitions_count=repetitions_count)

            #Writing the output frame to the video.
            output_video.write(cv2.cvtColor(np.array(output_frame),cv2.COLOR_RGB2BGR))

            frame_idx+=1
            pbar.update()




